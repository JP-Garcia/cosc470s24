{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"network3.py\n",
    "~~~~~~~~~~~~~~\n",
    "\n",
    "A Theano-based program for training and running simple neural\n",
    "networks.\n",
    "\n",
    "Supports several layer types (fully connected, convolutional, max\n",
    "pooling, softmax), and activation functions (sigmoid, tanh, and\n",
    "rectified linear units, with more easily added).\n",
    "\n",
    "When run on a CPU, this program is much faster than network.py and\n",
    "network2.py.  However, unlike network.py and network2.py it can also\n",
    "be run on a GPU, which makes it faster still.\n",
    "\n",
    "Because the code is based on Theano, the code is different in many\n",
    "ways from network.py and network2.py.  However, where possible I have\n",
    "tried to maintain consistency with the earlier programs.  In\n",
    "particular, the API is similar to network2.py.  Note that I have\n",
    "focused on making the code simple, easily readable, and easily\n",
    "modifiable.  It is not optimized, and omits many desirable features.\n",
    "\n",
    "This program incorporates ideas from the Theano documentation on\n",
    "convolutional neural nets (notably,\n",
    "http://deeplearning.net/tutorial/lenet.html ), from Misha Denil's\n",
    "implementation of dropout (https://github.com/mdenil/dropout ), and\n",
    "from Chris Olah (http://colah.github.io ).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv\n",
    "from theano.tensor.nnet import softmax\n",
    "from theano.tensor import shared_randomstreams\n",
    "from theano.tensor.signal.pool import pool_2d\n",
    "\n",
    "# Activation functions for neurons\n",
    "def linear(z): return z\n",
    "def ReLU(z): return T.maximum(0.0, z)\n",
    "from theano.tensor.nnet import sigmoid\n",
    "from theano.tensor import tanh\n",
    "\n",
    "\n",
    "#### Constants\n",
    "GPU = True\n",
    "if GPU:\n",
    "    print(\"Trying to run under a GPU.  If this is not desired, then modify \"+\\\n",
    "        \"network3.py\\nto set the GPU flag to False.\")\n",
    "    try: theano.config.device = 'gpu'\n",
    "    except: pass # it's already set\n",
    "    theano.config.floatX = 'float32'\n",
    "else:\n",
    "    print(\"Running with a CPU.  If this is not desired, then the modify \"+\\\n",
    "        \"network3.py to set\\nthe GPU flag to True.\")\n",
    "\n",
    "#### Load the MNIST data\n",
    "def load_data_shared(filename=\"mnist.pkl.gz\"):\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    def shared(data):\n",
    "        \"\"\"Place the data into shared variables.  This allows Theano to copy\n",
    "        the data to the GPU, if one is available.\n",
    "\n",
    "        \"\"\"\n",
    "        shared_x = theano.shared(\n",
    "            np.asarray(data[0], dtype=theano.config.floatX), borrow=True)\n",
    "        shared_y = theano.shared(\n",
    "            np.asarray(data[1], dtype=theano.config.floatX), borrow=True)\n",
    "        return shared_x, T.cast(shared_y, \"int32\")\n",
    "    return [shared(training_data), shared(validation_data), shared(test_data)]\n",
    "\n",
    "#### Main class used to construct and train networks\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, layers, mini_batch_size):\n",
    "        \"\"\"Takes a list of `layers`, describing the network architecture, and\n",
    "        a value for the `mini_batch_size` to be used during training\n",
    "        by stochastic gradient descent.\n",
    "\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.params = [param for layer in self.layers for param in layer.params]\n",
    "        self.x = T.matrix(\"x\")\n",
    "        self.y = T.ivector(\"y\")\n",
    "        init_layer = self.layers[0]\n",
    "        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
    "        for j in range(1, len(self.layers)): # xrange() was renamed to range() in Python 3.\n",
    "            prev_layer, layer  = self.layers[j-1], self.layers[j]\n",
    "            layer.set_inpt(\n",
    "                prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
    "        self.output = self.layers[-1].output\n",
    "        self.output_dropout = self.layers[-1].output_dropout\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            validation_data, test_data, lmbda=0.0):\n",
    "        \"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"\n",
    "        training_x, training_y = training_data\n",
    "        validation_x, validation_y = validation_data\n",
    "        test_x, test_y = test_data\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        num_training_batches = int(size(training_data)/mini_batch_size)\n",
    "        num_validation_batches = int(size(validation_data)/mini_batch_size)\n",
    "        num_test_batches = int(size(test_data)/mini_batch_size)\n",
    "\n",
    "        # define the (regularized) cost function, symbolic gradients, and updates\n",
    "        l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
    "        cost = self.layers[-1].cost(self)+\\\n",
    "               0.5*lmbda*l2_norm_squared/num_training_batches\n",
    "        grads = T.grad(cost, self.params)\n",
    "        updates = [(param, param-eta*grad)\n",
    "                   for param, grad in zip(self.params, grads)]\n",
    "\n",
    "        # define functions to train a mini-batch, and to compute the\n",
    "        # accuracy in validation and test mini-batches.\n",
    "        i = T.lscalar() # mini-batch index\n",
    "        train_mb = theano.function(\n",
    "            [i], cost, updates=updates,\n",
    "            givens={\n",
    "                self.x:\n",
    "                training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "                self.y:\n",
    "                training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        validate_mb_accuracy = theano.function(\n",
    "            [i], self.layers[-1].accuracy(self.y),\n",
    "            givens={\n",
    "                self.x:\n",
    "                validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "                self.y:\n",
    "                validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        test_mb_accuracy = theano.function(\n",
    "            [i], self.layers[-1].accuracy(self.y),\n",
    "            givens={\n",
    "                self.x:\n",
    "                test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
    "                self.y:\n",
    "                test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        self.test_mb_predictions = theano.function(\n",
    "            [i], self.layers[-1].y_out,\n",
    "            givens={\n",
    "                self.x:\n",
    "                test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
    "            })\n",
    "        # Do the actual training\n",
    "        best_validation_accuracy = 0.0\n",
    "        for epoch in range(epochs):\n",
    "            for minibatch_index in range(num_training_batches):\n",
    "                iteration = num_training_batches*epoch+minibatch_index\n",
    "                if iteration % 1000 == 0:\n",
    "                    print(\"Training mini-batch number {0}\".format(iteration))\n",
    "                cost_ij = train_mb(minibatch_index)\n",
    "                if (iteration+1) % num_training_batches == 0:\n",
    "                    validation_accuracy = np.mean(\n",
    "                        [validate_mb_accuracy(j) for j in range(num_validation_batches)])\n",
    "                    print(\"Epoch {0}: validation accuracy {1:.2%}\".format(\n",
    "                        epoch, validation_accuracy))\n",
    "                    if validation_accuracy >= best_validation_accuracy:\n",
    "                        print(\"This is the best validation accuracy to date.\")\n",
    "                        best_validation_accuracy = validation_accuracy\n",
    "                        best_iteration = iteration\n",
    "                        if test_data:\n",
    "                            test_accuracy = np.mean(\n",
    "                                [test_mb_accuracy(j) for j in range(num_test_batches)])\n",
    "                            print('The corresponding test accuracy is {0:.2%}'.format(\n",
    "                                test_accuracy))\n",
    "        print(\"Finished training network.\")\n",
    "        print(\"Best validation accuracy of {0:.2%} obtained at iteration {1}\".format(\n",
    "            best_validation_accuracy, best_iteration))\n",
    "        print(\"Corresponding test accuracy of {0:.2%}\".format(test_accuracy))\n",
    "\n",
    "#### Define layer types\n",
    "\n",
    "class ConvPoolLayer(object):\n",
    "    \"\"\"Used to create a combination of a convolutional and a max-pooling\n",
    "    layer.  A more sophisticated implementation would separate the\n",
    "    two, but for our purposes we'll always use them together, and it\n",
    "    simplifies the code, so it makes sense to combine them.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filter_shape, image_shape, poolsize=(2, 2),\n",
    "                 activation_fn=sigmoid):\n",
    "        \"\"\"`filter_shape` is a tuple of length 4, whose entries are the number\n",
    "        of filters, the number of input feature maps, the filter height, and the\n",
    "        filter width.\n",
    "\n",
    "        `image_shape` is a tuple of length 4, whose entries are the\n",
    "        mini-batch size, the number of input feature maps, the image\n",
    "        height, and the image width.\n",
    "\n",
    "        `poolsize` is a tuple of length 2, whose entries are the y and\n",
    "        x pooling sizes.\n",
    "\n",
    "        \"\"\"\n",
    "        self.filter_shape = filter_shape\n",
    "        self.image_shape = image_shape\n",
    "        self.poolsize = poolsize\n",
    "        self.activation_fn=activation_fn\n",
    "        # initialize weights and biases\n",
    "        n_out = (filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize))\n",
    "        self.w = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), size=filter_shape),\n",
    "                dtype=theano.config.floatX),\n",
    "            borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(loc=0, scale=1.0, size=(filter_shape[0],)),\n",
    "                dtype=theano.config.floatX),\n",
    "            borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "\n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        self.inpt = inpt.reshape(self.image_shape)\n",
    "        conv_out = conv.conv2d(\n",
    "            input=self.inpt, filters=self.w, filter_shape=self.filter_shape,\n",
    "            image_shape=self.image_shape)\n",
    "        pooled_out = pool_2d(\n",
    "            input=conv_out, ws=self.poolsize, ignore_border=True)\n",
    "        self.output = self.activation_fn(\n",
    "            pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "        self.output_dropout = self.output # no dropout in the convolutional layers\n",
    "\n",
    "class FullyConnectedLayer(object):\n",
    "\n",
    "    def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.activation_fn = activation_fn\n",
    "        self.p_dropout = p_dropout\n",
    "        # Initialize weights and biases\n",
    "        self.w = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(\n",
    "                    loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
    "                dtype=theano.config.floatX),\n",
    "            name='w', borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
    "                       dtype=theano.config.floatX),\n",
    "            name='b', borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "\n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
    "        self.output = self.activation_fn(\n",
    "            (1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
    "        self.y_out = T.argmax(self.output, axis=1)\n",
    "        self.inpt_dropout = dropout_layer(\n",
    "            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
    "        self.output_dropout = self.activation_fn(\n",
    "            T.dot(self.inpt_dropout, self.w) + self.b)\n",
    "\n",
    "    def accuracy(self, y):\n",
    "        \"Return the accuracy for the mini-batch.\"\n",
    "        return T.mean(T.eq(y, self.y_out))\n",
    "\n",
    "class SoftmaxLayer(object):\n",
    "\n",
    "    def __init__(self, n_in, n_out, p_dropout=0.0):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.p_dropout = p_dropout\n",
    "        # Initialize weights and biases\n",
    "        self.w = theano.shared(\n",
    "            np.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
    "            name='w', borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            np.zeros((n_out,), dtype=theano.config.floatX),\n",
    "            name='b', borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "\n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
    "        self.output = softmax((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
    "        self.y_out = T.argmax(self.output, axis=1)\n",
    "        self.inpt_dropout = dropout_layer(\n",
    "            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
    "        self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)\n",
    "\n",
    "    def cost(self, net):\n",
    "        \"Return the log-likelihood cost.\"\n",
    "        return -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[0]), net.y])\n",
    "\n",
    "    def accuracy(self, y):\n",
    "        \"Return the accuracy for the mini-batch.\"\n",
    "        return T.mean(T.eq(y, self.y_out))\n",
    "\n",
    "\n",
    "#### Miscellanea\n",
    "def size(data):\n",
    "    \"Return the size of the dataset `data`.\"\n",
    "    return data[0].get_value(borrow=True).shape[0]\n",
    "\n",
    "def dropout_layer(layer, p_dropout):\n",
    "    srng = shared_randomstreams.RandomStreams(\n",
    "        np.random.RandomState(0).randint(999999))\n",
    "    mask = srng.binomial(n=1, p=1-p_dropout, size=layer.shape)\n",
    "    return layer*T.cast(mask, theano.config.floatX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'theano'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Dev\\cosc-470\\cosc470s24-my_fork\\solutions\\lab 3\\lab3_sol.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Dev/cosc-470/cosc470s24-my_fork/solutions/lab%203/lab3_sol.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnetwork3\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Dev/cosc-470/cosc470s24-my_fork/solutions/lab%203/lab3_sol.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnetwork3\u001b[39;00m \u001b[39mimport\u001b[39;00m Network, ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer \u001b[39m# softmax plus log-likelihood cost is more common in modern image classification networks.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Dev/cosc-470/cosc470s24-my_fork/solutions/lab%203/lab3_sol.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# read data:\u001b[39;00m\n",
      "File \u001b[1;32md:\\Dev\\cosc-470\\cosc470s24-my_fork\\solutions\\lab 3\\network3.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39m# Third-party libraries\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtheano\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtheano\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensor\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mT\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtheano\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnnet\u001b[39;00m \u001b[39mimport\u001b[39;00m conv\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'theano'"
     ]
    }
   ],
   "source": [
    "import network3\n",
    "from network3 import Network, ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer # softmax plus log-likelihood cost is more common in modern image classification networks.\n",
    "\n",
    "# read data:\n",
    "training_data, validation_data, test_data = network3.load_data_shared()\n",
    "# mini-batch size:\n",
    "mini_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
